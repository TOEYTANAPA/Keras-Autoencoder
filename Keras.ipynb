{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "# import theano\n",
    "from PIL import Image\n",
    "from numpy import *\n",
    "# SKLEARN\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 50, 50\n",
    "\n",
    "# number of channels\n",
    "img_channels = 1\n",
    "\n",
    "#%%\n",
    "#  data\n",
    "# \n",
    "# path1 = 'data/train/cats'    #path of folder of images    \n",
    "path2 = 'data/train/resize50'  #path of folder to save images    \n",
    "\n",
    "\n",
    "# listing = os.listdir(path1) \n",
    "# num_samples=size(listing)\n",
    "# print (num_samples)\n",
    "num_samples = 24407\n",
    "# for file in listing:\n",
    "#     im = Image.open(path1 + '/' + file)   \n",
    "#     img = im.resize((img_rows,img_cols))\n",
    "#     gray = img.convert('L')\n",
    "#                 #need to do some more processing here           \n",
    "#     gray.save(path2 +'/' +  file, \"JPEG\")\n",
    "\n",
    "imlist = os.listdir(path2)\n",
    "\n",
    "\n",
    "im1 = array(Image.open('data/train/resize50' + '/'+ imlist[0])) # open one image to get size\n",
    "m,n = im1.shape[0:2] # get the size of the images\n",
    "imnbr = len(imlist) # get the number of images\n",
    "\n",
    "# create matrix to store all flattened images\n",
    "immatrix = array([array(Image.open('data/train/resize50'+ '/' + im2)).flatten()\n",
    "              for im2 in imlist],'f')\n",
    "                \n",
    "label=np.ones((num_samples,),dtype = int)\n",
    "label[0:12499]=0 \n",
    "label[12499:]=1\n",
    "\n",
    "\n",
    "data,Label = shuffle(immatrix,label, random_state=2)\n",
    "train_data = [data,Label]\n",
    "\n",
    "\n",
    "\n",
    "(X, y) = (train_data[0],train_data[1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (19525, 1, 50, 50)\n",
      "19525 train samples\n",
      "4882 test samples\n"
     ]
    }
   ],
   "source": [
    "(X, y) = (train_data[0],train_data[1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 100\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# Custom loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "y = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "\n",
    "\n",
    "# train the VAE on MNIST digits\n",
    "(X, y) = (train_data[0],train_data[1])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "vae.fit(x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called\n",
    "# X shape (60,000 28x28), y shape (10,000, )\n",
    "# (x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# train the VAE on MNIST digits\n",
    "(X, y) = (train_data[0],train_data[1])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "\n",
    "# data pre-processing\n",
    "# x_train = x_train.astype('float32') / 255. - 0.5       # minmax_normalized\n",
    "# x_test = x_test.astype('float32') / 255. - 0.5         # minmax_normalized\n",
    "# x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "# x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# in order to plot in a 2D figure\n",
    "encoding_dim = 2\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(2500,))\n",
    "\n",
    "# encoder layers\n",
    "encoded = Dense(1024, activation='relu')(input_img)\n",
    "encoded = Dense(512, activation='relu')(encoded)\n",
    "# encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='relu')(encoded)\n",
    "encoder_output = Dense(encoding_dim)(encoded)\n",
    "\n",
    "# decoder layers\n",
    "decoded = Dense(10, activation='relu')(encoder_output)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "# decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(512, activation='relu')(decoded)\n",
    "decoded = Dense(1024, activation='relu')(decoded)\n",
    "decoded = Dense(2500, activation='tanh')(decoded)\n",
    "\n",
    "# construct the autoencoder model\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "\n",
    "# construct the encoder model for plotting\n",
    "encoder = Model(input=input_img, output=encoder_output)\n",
    "\n",
    "# compile autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# training\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=200,\n",
    "                batch_size=245,\n",
    "                shuffle=True)\n",
    "\n",
    "# plotting\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import vae # this is our model - to be explored in the next post\n",
    "\n",
    "\n",
    "IMG_DIM = 28\n",
    "ARCHITECTURE = [IMG_DIM**2, # 784 pixels\n",
    "                500, 500, # intermediate encoding\n",
    "                50] # latent space dims\n",
    "# (and symmetrically back out again)\n",
    "HYPERPARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 1E-3,\n",
    "    \"dropout\": 0.9,\n",
    "    \"lambda_l2_reg\": 1E-5,\n",
    "    \"nonlinearity\": tf.nn.elu,\n",
    "    \"squashing\": tf.nn.sigmoid\n",
    "}\n",
    "\n",
    "mnist = input_data.read_data_sets(\"mnist_data\")\n",
    "v = vae.VAE(ARCHITECTURE, HYPERPARAMS)\n",
    "v.train(train_data, max_iter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers.merge import concatenate as concat\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imsave\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X, y) = (train_data[0],train_data[1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "n_pixels = np.prod(X_train.shape[1:])\n",
    "X_train = X_train.reshape((len(X_train), n_pixels))\n",
    "X_test = X_test.reshape((len(X_test), n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 250 # batch size\n",
    "n_z = 2 # latent space size\n",
    "encoder_dim1 = 512 # dim of encoder hidden layer\n",
    "#encoder_dim2 = 128 # dim of encoder hidden layer\n",
    "decoder_dim = 512 # dim of decoder hidden layer\n",
    "decoder_out_dim = 784 # dim of decoder output layer\n",
    "activ = 'relu'\n",
    "optim = Adam(lr=0.0005)\n",
    "\n",
    "\n",
    "n_x = X_train.shape[1]\n",
    "n_y = y_train.shape[1]\n",
    "\n",
    "\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Input(shape=(n_x,))\n",
    "label = Input(shape=(n_y,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = concat([X, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_h = Dense(encoder_dim1, activation=activ, activity_regularizer = 'l2')(inputs)\n",
    "#encoder_h = Dense(encoder_dim2, activation=activ)(encoder_h)\n",
    "mu = Dense(n_z, activation='linear')(encoder_h)\n",
    "l_sigma = Dense(n_z, activation='linear')(encoder_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "    mu, l_sigma = args\n",
    "    eps = K.random_normal(shape=(m, n_z), mean=0., stddev=1.)\n",
    "    return mu + K.exp(l_sigma / 2) * eps\n",
    "\n",
    "\n",
    "# Sampling latent space\n",
    "z = Lambda(sample_z, output_shape = (n_z, ))([mu, l_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = Lambda(sample_z, output_shape = (n_z, ))([mu, l_sigma])\n",
    "\n",
    "# merge latent space with label\n",
    "zc = concat([z, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_hidden = Dense(decoder_dim, activation=activ)\n",
    "decoder_out = Dense(decoder_out_dim, activation='sigmoid')\n",
    "h_p = decoder_hidden(zc)\n",
    "outputs = decoder_out(h_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n",
    "    kl = 0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1)\n",
    "    return recon + kl\n",
    "\n",
    "def KL_loss(y_true, y_pred):\n",
    "    return(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))\n",
    "\n",
    "def recon_loss(y_true, y_pred):\n",
    "    return(K.sum(K.binary_crossentropy(y_pred, y_true), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvae = Model([X, label], outputs)\n",
    "encoder = Model([X, label], mu)\n",
    "\n",
    "d_in = Input(shape=(n_z+n_y,))\n",
    "d_h = decoder_hidden(d_in)\n",
    "d_out = decoder_out(d_h)\n",
    "decoder = Model(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvae.compile(optimizer=optim, loss=vae_loss, metrics = [KL_loss, recon_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile and fit\n",
    "cvae_hist = cvae.fit([X_train, y_train], X_train, verbose = 0, batch_size=m, epochs=n_epoch,\n",
    "                            validation_data = ([X_test, y_test], X_test),\n",
    "                            callbacks = [EarlyStopping(patience = 5),\n",
    "                                         TQDMNotebookCallback(metric_format=\"{name}: {value:0.1f}\",\n",
    "                                                              leave_outer=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0].reshape(28, 28), cmap = plt.cm.gray), axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_train = encoder.predict([X_train, y_train])\n",
    "encodings= np.asarray(z_train)\n",
    "encodings = encodings.reshape(X_train.shape[0], n_z)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(encodings[:, 0], encodings[:, 1], c=Y_train, cmap=plt.cm.jet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19525, 2500)\n",
      "(4882, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "C:\\Users\\ta\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "19525/19525 [==============================] - 49s 3ms/step - loss: 0.0660\n",
      "Epoch 2/100\n",
      "19525/19525 [==============================] - 49s 3ms/step - loss: 0.0659\n",
      "Epoch 3/100\n",
      "19525/19525 [==============================] - 51s 3ms/step - loss: 0.0659\n",
      "Epoch 4/100\n",
      "19525/19525 [==============================] - 49s 3ms/step - loss: 0.0659\n",
      "Epoch 5/100\n",
      "19525/19525 [==============================] - 53s 3ms/step - loss: 0.0659\n",
      "Epoch 6/100\n",
      "19525/19525 [==============================] - 52s 3ms/step - loss: 0.0659\n",
      "Epoch 7/100\n",
      "19525/19525 [==============================] - 52s 3ms/step - loss: 0.0659\n",
      "Epoch 8/100\n",
      "19525/19525 [==============================] - 52s 3ms/step - loss: 0.0659\n",
      "Epoch 9/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 10/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 11/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 12/100\n",
      "19525/19525 [==============================] - 53s 3ms/step - loss: 0.0659\n",
      "Epoch 13/100\n",
      "19525/19525 [==============================] - 52s 3ms/step - loss: 0.0659\n",
      "Epoch 14/100\n",
      "19525/19525 [==============================] - 53s 3ms/step - loss: 0.0659\n",
      "Epoch 15/100\n",
      "19525/19525 [==============================] - 52s 3ms/step - loss: 0.0659\n",
      "Epoch 16/100\n",
      "19525/19525 [==============================] - 53s 3ms/step - loss: 0.0659\n",
      "Epoch 17/100\n",
      "19525/19525 [==============================] - 56s 3ms/step - loss: 0.0659\n",
      "Epoch 18/100\n",
      "19525/19525 [==============================] - 54s 3ms/step - loss: 0.0659\n",
      "Epoch 19/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 20/100\n",
      "19525/19525 [==============================] - 53s 3ms/step - loss: 0.0659\n",
      "Epoch 21/100\n",
      "19525/19525 [==============================] - 57s 3ms/step - loss: 0.0659\n",
      "Epoch 22/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 23/100\n",
      "19525/19525 [==============================] - 62s 3ms/step - loss: 0.0659\n",
      "Epoch 24/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 25/100\n",
      "19525/19525 [==============================] - 64s 3ms/step - loss: 0.0659\n",
      "Epoch 26/100\n",
      "19525/19525 [==============================] - 63s 3ms/step - loss: 0.0659\n",
      "Epoch 27/100\n",
      "19525/19525 [==============================] - 62s 3ms/step - loss: 0.0659\n",
      "Epoch 28/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 29/100\n",
      "19525/19525 [==============================] - 59s 3ms/step - loss: 0.0659\n",
      "Epoch 30/100\n",
      "19525/19525 [==============================] - 62s 3ms/step - loss: 0.0659\n",
      "Epoch 31/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 32/100\n",
      "19525/19525 [==============================] - 63s 3ms/step - loss: 0.0659\n",
      "Epoch 33/100\n",
      "19525/19525 [==============================] - 63s 3ms/step - loss: 0.0659\n",
      "Epoch 34/100\n",
      "19525/19525 [==============================] - 64s 3ms/step - loss: 0.0659\n",
      "Epoch 35/100\n",
      "19525/19525 [==============================] - 62s 3ms/step - loss: 0.0659\n",
      "Epoch 36/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 37/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 38/100\n",
      "19525/19525 [==============================] - 56s 3ms/step - loss: 0.0659\n",
      "Epoch 39/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 40/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 41/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 42/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 43/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 44/100\n",
      "19525/19525 [==============================] - 56s 3ms/step - loss: 0.0659\n",
      "Epoch 45/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 46/100\n",
      "19525/19525 [==============================] - 56s 3ms/step - loss: 0.0659\n",
      "Epoch 47/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 48/100\n",
      "19525/19525 [==============================] - 54s 3ms/step - loss: 0.0659\n",
      "Epoch 49/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 50/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 51/100\n",
      "19525/19525 [==============================] - 59s 3ms/step - loss: 0.0659\n",
      "Epoch 52/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 53/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 54/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 55/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 56/100\n",
      "19525/19525 [==============================] - 59s 3ms/step - loss: 0.0659\n",
      "Epoch 57/100\n",
      "19525/19525 [==============================] - 52s 3ms/step - loss: 0.0659\n",
      "Epoch 58/100\n",
      "19525/19525 [==============================] - 51s 3ms/step - loss: 0.0659\n",
      "Epoch 59/100\n",
      "19525/19525 [==============================] - 52s 3ms/step - loss: 0.0659\n",
      "Epoch 60/100\n",
      "19525/19525 [==============================] - 54s 3ms/step - loss: 0.0659\n",
      "Epoch 61/100\n",
      "19525/19525 [==============================] - 59s 3ms/step - loss: 0.0659\n",
      "Epoch 62/100\n",
      "19525/19525 [==============================] - 53s 3ms/step - loss: 0.0659\n",
      "Epoch 63/100\n",
      "19525/19525 [==============================] - 56s 3ms/step - loss: 0.0659\n",
      "Epoch 64/100\n",
      "19525/19525 [==============================] - 56s 3ms/step - loss: 0.0659\n",
      "Epoch 65/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 66/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 67/100\n",
      "19525/19525 [==============================] - 54s 3ms/step - loss: 0.0659\n",
      "Epoch 68/100\n",
      "19525/19525 [==============================] - 57s 3ms/step - loss: 0.0659\n",
      "Epoch 69/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 70/100\n",
      "19525/19525 [==============================] - 54s 3ms/step - loss: 0.0659\n",
      "Epoch 71/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 72/100\n",
      "19525/19525 [==============================] - 57s 3ms/step - loss: 0.0659\n",
      "Epoch 73/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 74/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 75/100\n",
      "19525/19525 [==============================] - 56s 3ms/step - loss: 0.0659\n",
      "Epoch 76/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 77/100\n",
      "19525/19525 [==============================] - 55s 3ms/step - loss: 0.0659\n",
      "Epoch 78/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 79/100\n",
      "19525/19525 [==============================] - 61s 3ms/step - loss: 0.0659\n",
      "Epoch 80/100\n",
      "19525/19525 [==============================] - 65s 3ms/step - loss: 0.0659\n",
      "Epoch 81/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 82/100\n",
      "19525/19525 [==============================] - 68s 3ms/step - loss: 0.0659\n",
      "Epoch 83/100\n",
      "19525/19525 [==============================] - 68s 3ms/step - loss: 0.0659\n",
      "Epoch 84/100\n",
      "19525/19525 [==============================] - 69s 4ms/step - loss: 0.0659\n",
      "Epoch 85/100\n",
      "19525/19525 [==============================] - 66s 3ms/step - loss: 0.0659\n",
      "Epoch 86/100\n",
      "19525/19525 [==============================] - 59s 3ms/step - loss: 0.0659\n",
      "Epoch 87/100\n",
      "19525/19525 [==============================] - 66s 3ms/step - loss: 0.0659\n",
      "Epoch 88/100\n",
      "19525/19525 [==============================] - 63s 3ms/step - loss: 0.0659\n",
      "Epoch 89/100\n",
      "19525/19525 [==============================] - 59s 3ms/step - loss: 0.0659\n",
      "Epoch 90/100\n",
      "19525/19525 [==============================] - 58s 3ms/step - loss: 0.0659\n",
      "Epoch 91/100\n",
      "19525/19525 [==============================] - 64s 3ms/step - loss: 0.0659\n",
      "Epoch 92/100\n",
      "19525/19525 [==============================] - 64s 3ms/step - loss: 0.0659\n",
      "Epoch 93/100\n",
      "19525/19525 [==============================] - 65s 3ms/step - loss: 0.0659\n",
      "Epoch 94/100\n",
      "19525/19525 [==============================] - 65s 3ms/step - loss: 0.0659\n",
      "Epoch 95/100\n",
      "19525/19525 [==============================] - 64s 3ms/step - loss: 0.0659\n",
      "Epoch 96/100\n",
      "19525/19525 [==============================] - 59s 3ms/step - loss: 0.0659\n",
      "Epoch 97/100\n",
      "19525/19525 [==============================] - 57s 3ms/step - loss: 0.0659\n",
      "Epoch 98/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 99/100\n",
      "19525/19525 [==============================] - 60s 3ms/step - loss: 0.0659\n",
      "Epoch 100/100\n",
      "19525/19525 [==============================] - 62s 3ms/step - loss: 0.0659\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1lBO8GZmDfX/Aer62NQWNzAAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a9d349ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called\n",
    "# X shape (60,000 28x28), y shape (10,000, )\n",
    "\n",
    "(X, y) = (train_data[0],train_data[1])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x_train = x_train.astype('float32') / 255.\n",
    "# x_test = x_test.astype('float32') / 255.\n",
    "# x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "# x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# (x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# data pre-processing\n",
    "x_train = x_train.astype('float32') / 255. - 0.5       # minmax_normalized\n",
    "x_test = x_test.astype('float32') / 255. - 0.5         # minmax_normalized\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# in order to plot in a 2D figure\n",
    "encoding_dim = 2\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(2500,))\n",
    "\n",
    "# encoder layers\n",
    "# encoded = Dense(32768, activation='relu')(input_img)\n",
    "# encoded = Dense(16384, activation='relu')(encoded)\n",
    "# encoded = Dense(8192, activation='relu')(input_img)\n",
    "encoded = Dense(2048, activation='relu')(input_img)\n",
    "encoded = Dense(1024, activation='relu')(encoded)\n",
    "encoded = Dense(512, activation='relu')(encoded)\n",
    "encoded = Dense(256, activation='relu')(encoded)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='relu')(encoded)\n",
    "# encoded = Dense(2, activation='relu')(encoded)\n",
    "encoder_output = Dense(encoding_dim)(encoded)\n",
    "\n",
    "# # decoder layers\n",
    "# decoded = Dense(2, activation='relu')(encoder_output)\n",
    "decoded = Dense(10, activation='relu')(decoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(512, activation='relu')(decoded)\n",
    "decoded = Dense(1024, activation='relu')(decoded)\n",
    "decoded = Dense(2048, activation='relu')(decoded)\n",
    "decoded = Dense(2500, activation='relu')(decoded)\n",
    "# # decoded = Dense(8192, activation='relu')(decoded)\n",
    "# decoded = Dense(10000, activation='relu')(decoded)\n",
    "# # decoded = Dense(32768, activation='relu')(decoded)\n",
    "# # decoded = Dense(40000, activation='tanh')(decoded)\n",
    "\n",
    "# construct the autoencoder model\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "\n",
    "# construct the encoder model for plotting\n",
    "encoder = Model(input=input_img, output=encoder_output)\n",
    "\n",
    "# compile autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# training\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=255,\n",
    "                shuffle=True)\n",
    "\n",
    "# plotting\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
